{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "colab": {
      "name": "TTTP6234_Restaurant_Assignment3_P101447_HOOYT.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NQzndHpNWnAt",
        "colab_type": "text"
      },
      "source": [
        "# Assignment #3\n",
        "## Topic: Restaurant Reviews - \n",
        "Name: **Hoo Yee Torng** </br>\n",
        "Matrix: **P101447**\n",
        "\n",
        "## 1.0 IMPORT LIBRARY\n",
        "Import all the library needed and download the "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tRJqjYPoWnAw",
        "colab_type": "code",
        "outputId": "e7c0f800-3ad9-46b8-c560-e163bf0a216a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 181
        }
      },
      "source": [
        "import pandas as pd\n",
        "import nltk\n",
        "import time\n",
        "import numpy as np\n",
        "import re\n",
        "import spacy\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.naive_bayes import GaussianNB, MultinomialNB\n",
        "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier\n",
        "from sklearn import svm\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "from nltk.corpus import stopwords\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "stop_words = set(stopwords.words('english'))\n",
        "nltk.download('averaged_perceptron_tagger')"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7u0R6AuBWnA2",
        "colab_type": "text"
      },
      "source": [
        "## 2.0 Load Dataset files to be processed\n",
        "download following files and upload to the workspace\n",
        "- [restaurant_positive_reviews.csv](https://drive.google.com/open?id=1JfdkZuWwsMAdbZk66BeY-jm9YJIu8nEp)\n",
        "- [restaurant_negative_reviews.csv](https://drive.google.com/open?id=1h2_PDZOnFf8bez_Zm2EJ01gP4sL94b-x) <br/>\n",
        "\n",
        "Create a combined master set of data with class **0 as Negative** review and **1 as Positive**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "o9xfoBVDWnA3",
        "colab_type": "code",
        "outputId": "8ca33979-4c67-4ba0-a932-0288b706784d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        }
      },
      "source": [
        "# READ FROM NEGATIVE REVIEW AND ASSIGN 0 AS CLASS VALUE\n",
        "negative_df = pd.read_csv('restaurant_negative_review.csv')\n",
        "negative_df[\"label\"] = 0\n",
        "print(negative_df.shape)\n",
        "\n",
        "# READ FROM POSITIVE REVIEW AND ASSIGN 1 AS CLASS VALUE\n",
        "positive_df = pd.read_csv('restaurant_positive_review.csv')\n",
        "positive_df[\"label\"] = 1\n",
        "print(positive_df.shape)\n",
        "\n",
        "# APPEND BOTH DATAFRAME AS 1\n",
        "raw_df = positive_df.append(negative_df) \n",
        "\n",
        "# RENAME COLUMN\n",
        "raw_df = raw_df.rename(columns={\"Google Translate\": \"review\"})\n",
        "print(raw_df.shape)\n",
        "\n",
        "raw_df.index = range(134)\n",
        "raw_df['review'].apply(lambda x: len(x.split(' '))).sum()\n"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(50, 2)\n",
            "(84, 2)\n",
            "(134, 2)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3701"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XLxOHMSFWnBI",
        "colab_type": "code",
        "outputId": "c6226a24-b403-444d-804f-532119640d65",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        }
      },
      "source": [
        "cnt_pro = raw_df['label'].value_counts()\n",
        "plt.figure(figsize=(12,4))\n",
        "sns.barplot(cnt_pro.index, cnt_pro.values, alpha=0.8)\n",
        "plt.ylabel('Politic Review', fontsize=12)\n",
        "plt.xlabel('Labels', fontsize=12)\n",
        "plt.xticks(rotation=90)\n",
        "plt.show();"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAtAAAAEGCAYAAABM2KIzAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRK5CYII=\n",
            "text/plain": [
              "<Figure size 864x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2o4ewuXLWnBP",
        "colab_type": "text"
      },
      "source": [
        "## 3.0 PRE-PROCESSING"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9IhqS8tFUegB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def extract_words(setence):\n",
        "  text = re.sub(\"[^a-zA-Z]\", \" \", setence)\n",
        "  return(text.lower())\n",
        "\n",
        "def clean_word(sentence):\n",
        "  text = re.sub(r'\\|\\|\\|', r' ', sentence) \n",
        "  text = re.sub(r'http\\S+', r'<URL>', text)\n",
        "  text = text.replace('x', '')\n",
        "  return text\n",
        "\n",
        "def remove_stopword(sentence):\n",
        "    new_line = []\n",
        "    word_tokens = nltk.word_tokenize(sentence)\n",
        "    for w in word_tokens:\n",
        "      if w not in stop_words:\n",
        "        new_line.append(w)\n",
        "    \n",
        "    return (\" \".join(new_line) + \" \").strip()\n",
        "\n",
        "def get_adjective_word(setence):\n",
        "  words = []\n",
        "\n",
        "  ## CHECK BY SPACY FIRST\n",
        "  spacy_words = nlp(u''+setence+'')\n",
        "  for token in spacy_words:\n",
        "    if token.pos_ == 'ADJ':\n",
        "      #print(\"SPACY: {}:{}\".format(token.text, token.pos_))\n",
        "      words.append(token.text)\n",
        "\n",
        "  ## CHECK BY NLTK\n",
        "  nltk_words = nltk.word_tokenize(setence)\n",
        "  for stc in nltk_words:\n",
        "    nltk_token = nltk.pos_tag(nltk.word_tokenize(stc))\n",
        "    if nltk_token[0][1] == 'JJ' and nltk_token[0][0] not in words:\n",
        "      #print(\"NTLK : {}:{}\".format(nltk_token[0][0], nltk_token[0][1]))\n",
        "      words.append(nltk_token[0][0])\n",
        "    \n",
        "  return (\" \".join(words) + \" \").strip()\n",
        "\n",
        "def text_processing(objdf):\n",
        "  for index, row in objdf.iterrows():\n",
        "    selected_review = row['review'].strip()\n",
        "    #print(\"--\")\n",
        "    #print(selected_review)\n",
        "    selected_review = extract_words(selected_review)\n",
        "    selected_review = clean_word(selected_review)\n",
        "    selected_review = remove_stopword(selected_review)\n",
        "    #selected_review = get_adjective_word(selected_review)\n",
        "    objdf.loc[index, \"review\"] = selected_review \n",
        "    #print(objdf.loc[index, \"review\"])\n",
        "  return objdf\n",
        "\n",
        "df = text_processing(raw_df)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OaTfnYYJcgFK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def show_percentage(x):\n",
        "  return \"{0:.2f}%\".format(round(x, 2) * 100)\n",
        "\n",
        "def run_ML(feature_name, xtrain, ytrain, xtest, ytest):\n",
        "\n",
        "  print(\"-----------------------------------------------------------------------------\")\n",
        "  print(\"- \" + feature_name)\n",
        "  print(\"-----------------------------------------------------------------------------\")\n",
        "\n",
        "  if feature_name == \"Doc2Vec\":\n",
        "    clf_dict = {\n",
        "      'GradientBoostingClassifier': GradientBoostingClassifier(n_estimators=100),\n",
        "      'RandomForestClassifier': RandomForestClassifier(n_estimators=100, random_state=0),\n",
        "      'GaussianNB': GaussianNB(),\n",
        "      'ANN': MLPClassifier(solver='adam', hidden_layer_sizes=(35,15), random_state=2, activation='relu', max_iter=5000, learning_rate='invscaling'),\n",
        "      'SVN': svm.SVC(C=1.0, kernel='rbf', degree=3, gamma='scale', coef0=0.0, shrinking=True, probability=False, tol=0.001, cache_size=200, class_weight=None, verbose=False, max_iter=2000, decision_function_shape='ovr', random_state=2),\n",
        "      'LinearSVC': LinearSVC(penalty='l2', loss='squared_hinge', dual=True, tol=0.001, C=1.0, multi_class='ovr', fit_intercept=True, intercept_scaling=1, class_weight=None, verbose=0, random_state=None, max_iter=30000),\n",
        "      'LogisticRegression': LogisticRegression(random_state=42, max_iter=8000, multi_class='auto', solver='saga'),\n",
        "    }    \n",
        "  else:\n",
        "    clf_dict = {\n",
        "      'GradientBoostingClassifier': GradientBoostingClassifier(n_estimators=100),\n",
        "      'RandomForestClassifier': RandomForestClassifier(n_estimators=100, random_state=0),\n",
        "      'MultinomialNB': MultinomialNB(),\n",
        "      'ANN': MLPClassifier(solver='adam', hidden_layer_sizes=(35,15), random_state=2, activation='relu', max_iter=5000, learning_rate='invscaling'),\n",
        "      'SVN': svm.SVC(C=1.0, kernel='rbf', degree=3, gamma='scale', coef0=0.0, shrinking=True, probability=False, tol=0.001, cache_size=200, class_weight=None, verbose=False, max_iter=2000, decision_function_shape='ovr', random_state=2),\n",
        "      'LinearSVC': LinearSVC(penalty='l2', loss='squared_hinge', dual=True, tol=0.001, C=1.0, multi_class='ovr', fit_intercept=True, intercept_scaling=1, class_weight=None, verbose=0, random_state=None, max_iter=30000),\n",
        "      'LogisticRegression': LogisticRegression(random_state=42, max_iter=8000, multi_class='auto', solver='saga'),\n",
        "    }\n",
        "\n",
        "  for name, clf in clf_dict.items():\n",
        "    clf.fit(xtrain, ytrain) \n",
        "    pred = clf.predict(xtest)\n",
        "    print(\"-- \" + name)\n",
        "    print('Testing accuracy: {}'.format(show_percentage(accuracy_score(ytest, pred))))\n",
        "    print('Testing F1 score: {}'.format(show_percentage(f1_score(ytest, pred, average='macro'))))\n",
        "    print('Testing Precision score: {}'.format(show_percentage(precision_score(ytest, pred, average='macro'))))\n",
        "    print('Testing Recall score: {} \\n'.format(show_percentage(recall_score(ytest, pred, average='macro'))))    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bzR6Ep1hWnBn",
        "colab_type": "code",
        "outputId": "03db1ca1-6dd5-4be5-985a-8182b95394e4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        }
      },
      "source": [
        "features = df['review'].values\n",
        "classes = df['label'].values\n",
        "\n",
        "sentences_train, sentences_test, yy_train, yy_test = train_test_split(features, classes, test_size=0.1, random_state=42, shuffle=True)\n",
        "\n",
        "print('Number of rows in the total set: {}'.format(df.shape[0]))\n",
        "print('Number of rows in the training set: {}'.format(sentences_train.shape[0]))\n",
        "print('Number of rows in the test set: {}'.format(sentences_test.shape[0]))"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of rows in the total set: 134\n",
            "Number of rows in the training set: 120\n",
            "Number of rows in the test set: 14\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SDVKSbluWnBm",
        "colab_type": "text"
      },
      "source": [
        "## 4.1 FEATURE - BOW"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AGgzmvqqg-QQ",
        "colab_type": "code",
        "outputId": "5f2da15c-ed4f-4a9a-ed95-54c496db1b11",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 835
        }
      },
      "source": [
        "BOW = CountVectorizer()\n",
        "BOW.fit(sentences_train)\n",
        "XX_train = BOW.transform(sentences_train)\n",
        "XX_test  = BOW.transform(sentences_test)\n",
        "run_ML(\"BOW\", XX_train, yy_train, XX_test, yy_test)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "-----------------------------------------------------------------------------\n",
            "- BOW\n",
            "-----------------------------------------------------------------------------\n",
            "-- GradientBoostingClassifier\n",
            "Testing accuracy: 79.00%\n",
            "Testing F1 score: 78.00%\n",
            "Testing Precision score: 77.00%\n",
            "Testing Recall score: 79.00% \n",
            "\n",
            "-- RandomForestClassifier\n",
            "Testing accuracy: 57.00%\n",
            "Testing F1 score: 48.00%\n",
            "Testing Precision score: 48.00%\n",
            "Testing Recall score: 49.00% \n",
            "\n",
            "-- MultinomialNB\n",
            "Testing accuracy: 86.00%\n",
            "Testing F1 score: 85.00%\n",
            "Testing Precision score: 86.00%\n",
            "Testing Recall score: 89.00% \n",
            "\n",
            "-- ANN\n",
            "Testing accuracy: 100.00%\n",
            "Testing F1 score: 100.00%\n",
            "Testing Precision score: 100.00%\n",
            "Testing Recall score: 100.00% \n",
            "\n",
            "-- SVN\n",
            "Testing accuracy: 71.00%\n",
            "Testing F1 score: 58.00%\n",
            "Testing Precision score: 85.00%\n",
            "Testing Recall score: 60.00% \n",
            "\n",
            "-- LinearSVC\n",
            "Testing accuracy: 86.00%\n",
            "Testing F1 score: 84.00%\n",
            "Testing Precision score: 84.00%\n",
            "Testing Recall score: 84.00% \n",
            "\n",
            "-- LogisticRegression\n",
            "Testing accuracy: 79.00%\n",
            "Testing F1 score: 75.00%\n",
            "Testing Precision score: 78.00%\n",
            "Testing Recall score: 74.00% \n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E2gjWIknWnBv",
        "colab_type": "text"
      },
      "source": [
        "## 4.2 FEATURE-TF - IDF"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FlGFChWNWnBw",
        "colab_type": "code",
        "outputId": "68bdf938-68e6-4f53-a52d-f654236f330f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 835
        }
      },
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "TFIDF = TfidfVectorizer()\n",
        "TFIDF.fit_transform(sentences_train)\n",
        "XXX_train = TFIDF.transform(sentences_train)\n",
        "XXX_test  = TFIDF.transform(sentences_test)\n",
        "\n",
        "run_ML(\"TF-IDF\", XXX_train, yy_train, XXX_test, yy_test)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "-----------------------------------------------------------------------------\n",
            "- TF-IDF\n",
            "-----------------------------------------------------------------------------\n",
            "-- GradientBoostingClassifier\n",
            "Testing accuracy: 79.00%\n",
            "Testing F1 score: 78.00%\n",
            "Testing Precision score: 77.00%\n",
            "Testing Recall score: 79.00% \n",
            "\n",
            "-- RandomForestClassifier\n",
            "Testing accuracy: 64.00%\n",
            "Testing F1 score: 52.00%\n",
            "Testing Precision score: 58.00%\n",
            "Testing Recall score: 54.00% \n",
            "\n",
            "-- MultinomialNB\n",
            "Testing accuracy: 71.00%\n",
            "Testing F1 score: 58.00%\n",
            "Testing Precision score: 85.00%\n",
            "Testing Recall score: 60.00% \n",
            "\n",
            "-- ANN\n",
            "Testing accuracy: 93.00%\n",
            "Testing F1 score: 93.00%\n",
            "Testing Precision score: 92.00%\n",
            "Testing Recall score: 94.00% \n",
            "\n",
            "-- SVN\n",
            "Testing accuracy: 86.00%\n",
            "Testing F1 score: 82.00%\n",
            "Testing Precision score: 91.00%\n",
            "Testing Recall score: 80.00% \n",
            "\n",
            "-- LinearSVC\n",
            "Testing accuracy: 93.00%\n",
            "Testing F1 score: 93.00%\n",
            "Testing Precision score: 92.00%\n",
            "Testing Recall score: 94.00% \n",
            "\n",
            "-- LogisticRegression\n",
            "Testing accuracy: 79.00%\n",
            "Testing F1 score: 71.00%\n",
            "Testing Precision score: 88.00%\n",
            "Testing Recall score: 70.00% \n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "luCQm8-V_VlU",
        "colab_type": "text"
      },
      "source": [
        "## 4.3 Feature - Doc2Vec"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p0Yvw434AQcm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import multiprocessing\n",
        "from gensim.models import Doc2Vec\n",
        "from gensim.models.doc2vec import TaggedDocument\n",
        "cores = multiprocessing.cpu_count()\n",
        "from tqdm import tqdm\n",
        "from sklearn import utils\n",
        "\n",
        "def tokenize_text(text):\n",
        "    tokens = []\n",
        "    for sent in nltk.sent_tokenize(text):\n",
        "        for word in nltk.word_tokenize(sent):\n",
        "            if len(word) < 2:\n",
        "                continue\n",
        "            tokens.append(word.lower())\n",
        "    return tokens"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CmHRO1nj_fY5",
        "colab_type": "code",
        "outputId": "af186b79-3aaf-4612-f2ca-cbf0d7fb5139",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "# SPLIT TO 9:1\n",
        "doc2vec_train, doc2vec_test = train_test_split(df, test_size=0.1, random_state=42, shuffle=True)\n",
        "\n",
        "# CONVERT TO DOC TO VEC FORMAT\n",
        "train_tagged = doc2vec_train.apply(lambda r: TaggedDocument(words=tokenize_text(r['review']), tags=[r.label]), axis=1)\n",
        "test_tagged = doc2vec_test.apply(lambda r: TaggedDocument(words=tokenize_text(r['review']), tags=[r.label]), axis=1)\n",
        "\n",
        "\n",
        "# dm :: dm=1, 'distributed memory' (PV-DM), 0=BOW\n",
        "# vector_size :: Dimensionality of the feature vectors\n",
        "# negative ::  0 = no negative sampling is used.\n",
        "# hs :: If 1, hierarchical softmax will be used for model training. If set to 0, and negative is non-zero, negative sampling will be used.\n",
        "# min_count (int, optional) – Ignores all words with total frequency lower than this.\n",
        "# sample (float, optional) – The threshold for configuring which higher-frequency words are randomly downsampled, useful range is (0, 1e-5).\n",
        "# workers (int, optional) – Use these many worker threads to train the model (=faster training with multicore machines).\n",
        "\n",
        "model_dbow = Doc2Vec(dm=0, vector_size=300, negative=5, hs=0, min_count=2, sample=0, workers=cores)\n",
        "model_dbow.build_vocab([x for x in tqdm(train_tagged.values)])\n"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 120/120 [00:00<00:00, 322638.77it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mSBx7K7OvuI5",
        "colab_type": "code",
        "outputId": "7884d7a4-db8e-4d3a-efaf-517791d5eb79",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 599
        }
      },
      "source": [
        "%%time\n",
        "for epoch in range(30):\n",
        "    model_dbow.train(utils.shuffle([x for x in tqdm(train_tagged.values)]), total_examples=len(train_tagged.values), epochs=100)\n",
        "    model_dbow.alpha -= 0.002\n",
        "    model_dbow.min_alpha = model_dbow.alpha"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 120/120 [00:00<00:00, 264485.80it/s]\n",
            "100%|██████████| 120/120 [00:00<00:00, 53210.33it/s]\n",
            "100%|██████████| 120/120 [00:00<00:00, 52571.18it/s]\n",
            "100%|██████████| 120/120 [00:00<00:00, 41380.95it/s]\n",
            "100%|██████████| 120/120 [00:00<00:00, 32459.47it/s]\n",
            "100%|██████████| 120/120 [00:00<00:00, 142906.44it/s]\n",
            "100%|██████████| 120/120 [00:00<00:00, 29975.37it/s]\n",
            "100%|██████████| 120/120 [00:00<00:00, 33266.13it/s]\n",
            "100%|██████████| 120/120 [00:00<00:00, 119212.81it/s]\n",
            "100%|██████████| 120/120 [00:00<00:00, 201972.91it/s]\n",
            "100%|██████████| 120/120 [00:00<00:00, 244684.73it/s]\n",
            "100%|██████████| 120/120 [00:00<00:00, 37139.65it/s]\n",
            "100%|██████████| 120/120 [00:00<00:00, 32413.48it/s]\n",
            "100%|██████████| 120/120 [00:00<00:00, 31516.37it/s]\n",
            "100%|██████████| 120/120 [00:00<00:00, 30667.59it/s]\n",
            "100%|██████████| 120/120 [00:00<00:00, 20276.21it/s]\n",
            "100%|██████████| 120/120 [00:00<00:00, 30077.48it/s]\n",
            "100%|██████████| 120/120 [00:00<00:00, 33897.93it/s]\n",
            "100%|██████████| 120/120 [00:00<00:00, 296242.78it/s]\n",
            "100%|██████████| 120/120 [00:00<00:00, 109416.63it/s]\n",
            "100%|██████████| 120/120 [00:00<00:00, 35385.02it/s]\n",
            "100%|██████████| 120/120 [00:00<00:00, 34788.26it/s]\n",
            "100%|██████████| 120/120 [00:00<00:00, 25407.19it/s]\n",
            "100%|██████████| 120/120 [00:00<00:00, 300846.67it/s]\n",
            "100%|██████████| 120/120 [00:00<00:00, 43778.07it/s]\n",
            "100%|██████████| 120/120 [00:00<00:00, 34093.10it/s]\n",
            "100%|██████████| 120/120 [00:00<00:00, 33614.94it/s]\n",
            "100%|██████████| 120/120 [00:00<00:00, 284038.65it/s]\n",
            "100%|██████████| 120/120 [00:00<00:00, 37219.29it/s]\n",
            "100%|██████████| 120/120 [00:00<00:00, 36522.49it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 16.4 s, sys: 990 ms, total: 17.4 s\n",
            "Wall time: 18.3 s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n_3pv7dawC0N",
        "colab_type": "code",
        "outputId": "fa96e274-3214-4ee0-ee56-a41bd6fe0e48",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 928
        }
      },
      "source": [
        "def vec_for_learning(model, tagged_docs):\n",
        "    sents = tagged_docs.values\n",
        "    targets, regressors = zip(*[(doc.tags[0], model.infer_vector(doc.words, steps=20)) for doc in sents])\n",
        "    return targets, regressors\n",
        "\n",
        "y_train, X_train = vec_for_learning(model_dbow, train_tagged)\n",
        "y_test, X_test = vec_for_learning(model_dbow, test_tagged)\n",
        "\n",
        "run_ML(\"Doc2Vec\", X_train, y_train, X_test, y_test)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "-----------------------------------------------------------------------------\n",
            "- Doc2Vec\n",
            "-----------------------------------------------------------------------------\n",
            "-- GradientBoostingClassifier\n",
            "Testing accuracy: 43.00%\n",
            "Testing F1 score: 38.00%\n",
            "Testing Precision score: 38.00%\n",
            "Testing Recall score: 38.00% \n",
            "\n",
            "-- RandomForestClassifier\n",
            "Testing accuracy: 43.00%\n",
            "Testing F1 score: 30.00%\n",
            "Testing Precision score: 27.00%\n",
            "Testing Recall score: 33.00% \n",
            "\n",
            "-- GaussianNB\n",
            "Testing accuracy: 36.00%\n",
            "Testing F1 score: 35.00%\n",
            "Testing Precision score: 40.00%\n",
            "Testing Recall score: 41.00% \n",
            "\n",
            "-- ANN\n",
            "Testing accuracy: 57.00%\n",
            "Testing F1 score: 36.00%\n",
            "Testing Precision score: 31.00%\n",
            "Testing Recall score: 44.00% \n",
            "\n",
            "-- SVN\n",
            "Testing accuracy: 64.00%\n",
            "Testing F1 score: 39.00%\n",
            "Testing Precision score: 32.00%\n",
            "Testing Recall score: 50.00% \n",
            "\n",
            "-- LinearSVC\n",
            "Testing accuracy: 57.00%\n",
            "Testing F1 score: 36.00%\n",
            "Testing Precision score: 31.00%\n",
            "Testing Recall score: 44.00% \n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "-- LogisticRegression\n",
            "Testing accuracy: 64.00%\n",
            "Testing F1 score: 39.00%\n",
            "Testing Precision score: 32.00%\n",
            "Testing Recall score: 50.00% \n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UsS02GXbWnB2",
        "colab_type": "text"
      },
      "source": [
        "### REPORT :<br>\n",
        "\n",
        "#### BOW\n",
        "\n",
        "| Classifier | Accuracy | F1 | Precision | Recall | \n",
        "| ------------- | ------------- | ------------- | ------------- | ------------- |\n",
        "| Gradient Boosting | 79.00% | 78.00% | 77.00% | 79.00% |\n",
        "| Random Forest | 57.00% | 48.00% | 48.00% | 49.00% |\n",
        "| Multinomial NB | 86.00% | 85.00% | 86.00% | 89.00% |\n",
        "| ANN | 100.00% | 100.00% | 100.00% | 100.00% |\n",
        "| SVN | 71.00% | 58.00% | 85.00% | 60.00% |\n",
        "| Linear | 86.00% | 84.00% | 84.00% | 84.00% |\n",
        "| Logistic Regression | 79.00% | 75.00% | 78.00% | 74.00% |\n",
        "\n",
        "#### TF-IDF\n",
        "\n",
        "| Classifier | Accuracy | F1 | Precision | Recall | \n",
        "| ------------- | ------------- | ------------- | ------------- | ------------- |\n",
        "| Gradient Boosting | 79.00% | 78.00% | 77.00% | 79.00% |\n",
        "| Random Forest | 64.00% | 52.00% | 58.00% | 54.00% |\n",
        "| Multinomial NB | 71.00% | 58.00% | 85.00% | 60.00% |\n",
        "| ANN | 93.00% | 93.00% | 92.00% | 94.00% |\n",
        "| SVN | 86.00% | 82.00% | 91.00% | 80.00% |\n",
        "| Linear | 93.00% | 93.00% | 92.00% | 84.00% |\n",
        "| Logistic Regression | 79.00% | 71.00% | 88.00% | 70.00% |\n",
        "\n",
        "#### DOC2VEC\n",
        "\n",
        "| Classifier | Accuracy | F1 | Precision | Recall | \n",
        "| ------------- | ------------- | ------------- | ------------- | ------------- |\n",
        "| Gradient Boosting | 43.00% | 38.00% | 38.00% | 38.00% |\n",
        "| Random Forest | 43.00% | 30.00% | 27.00% | 33.00% |\n",
        "| Gaussian NB | 36.00% | 35.00% | 40.00% | 41.00% |\n",
        "| ANN | 57.00% | 36.00% | 31.00% | 44.00% |\n",
        "| SVN | 64.00% | 39.00% | 32.00% | 50.00% |\n",
        "| Linear | 57.00% | 36.00% | 31.00% | 44.00% |\n",
        "| Logistic Regression | 64.00% | 39.00% | 32.00% | 50.00% |\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SRNdnL2UdCsl",
        "colab_type": "text"
      },
      "source": [
        "**Answer**\n",
        "* #1 - BOW with ANN\n",
        "* #2 - TD-IDF with ANN\n",
        "* #3 - TD-IDF with Linear\n",
        "\n"
      ]
    }
  ]
}
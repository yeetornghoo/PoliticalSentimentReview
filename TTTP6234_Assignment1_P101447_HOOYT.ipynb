{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TTTP6234 - Assignment1_P101447_HOOYT.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O-ZHDwP5nCmZ",
        "colab_type": "text"
      },
      "source": [
        "# Assignment #1\n",
        "## Topic: Political Reviews - Generate Lexicon Sentiment\n",
        "\n",
        "Name: **Hoo Yee Torng** </br>\n",
        "Matrix: **P101447**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E0NYnTZz2mXS",
        "colab_type": "text"
      },
      "source": [
        "## 1.0 IMPORT LIBRARY\n",
        "Import all the library needed and download the database for Spacy, NLTK Lemmatizer, Stopwords etc"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o0nn6Uw9m67n",
        "colab_type": "code",
        "outputId": "d411cf37-4540-4b3a-d576-c33444b68a91",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        }
      },
      "source": [
        "import pandas as pd\n",
        "import nltk\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.stem import LancasterStemmer\n",
        "from nltk.corpus import stopwords\n",
        "import re\n",
        "import spacy\n",
        "\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2cciHekQrpqO",
        "colab_type": "text"
      },
      "source": [
        "## 2.0 TEXT PROCESSING FUNCTION\n",
        "Define text processig function \n",
        "* extract_words - Only return alphebert words in lower case\n",
        "* get_adjective - Use both Spacy and NTLK to return adjective words\n",
        "* lemmatized_remove_stopword - Lemmatized the words and remove stop words\n",
        "* is_word_exist - To check if the word exist in the word list\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AhdNDhfz23I8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# RETURN WORD ONLY\n",
        "def extract_words(setence):\n",
        "  letters = re.sub(\"[^a-zA-Z]\", \" \", setence)\n",
        "  return(letters.lower())\n",
        "\n",
        "\n",
        "# GET ADJECTIVE ONLY\n",
        "# USE BOTH SPACY AND NTLK\n",
        "def get_adjective_word(setence):\n",
        "  words = []\n",
        "\n",
        "  ## CHECK BY SPACY FIRST\n",
        "  spacy_words = nlp(u''+setence+'')\n",
        "  for token in spacy_words:\n",
        "    if token.pos_ == 'ADV':\n",
        "      words.append(token.text)\n",
        "\n",
        "  ## CHECK BY NLTK\n",
        "  nltk_words = nltk.word_tokenize(setence)\n",
        "  for stc in nltk_words:\n",
        "    nltk_token = nltk.pos_tag(nltk.word_tokenize(stc))\n",
        "    if nltk_token[0][1] == 'JJ' and nltk_token[0][0] not in words:\n",
        "      words.append(nltk_token[0][0])\n",
        "    \n",
        "  return words\n",
        "\n",
        "\n",
        "# LEMMATIZED AND REMOVE STOP WORD\n",
        "def lemmatized_remove_stopword(sentence):\n",
        "    new_line = []\n",
        "    word_tokens = nltk.word_tokenize(sentence)\n",
        "    for w in word_tokens:\n",
        "      output_word = lemmatizer.lemmatize(w)\n",
        "      if output_word not in stop_words:\n",
        "        new_line.append(output_word)\n",
        "    \n",
        "    return (\" \".join(new_line) + \" \").strip()\n",
        "\n",
        "def is_word_exist(word, words):\n",
        "  for i in range(len(words)):\n",
        "    savedword = words[i][0].strip()\n",
        "    if savedword == word.strip():\n",
        "      return True\n",
        "  \n",
        "  return False"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ScOE10rc3sIz",
        "colab_type": "text"
      },
      "source": [
        "### 3.0 GENERATE LEXICON SENTIMENT FILE\n",
        "This is the function to generate the lexicon sentiment csv file"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YLRj1pC4tZca",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_lexicon_sentiment(source_df, score, export_file_name):\n",
        "\n",
        "  words = []\n",
        "\n",
        "  ttl_adj_words = 0\n",
        "  ttl_words = 0\n",
        "\n",
        "  for i in range(len(source_df)):\n",
        "    input_review = source_df.loc[i, \"original\"]\n",
        "    input_review = extract_words(input_review)\n",
        "    input_review = lemmatized_remove_stopword(input_review)\n",
        "    input_words = get_adjective_word(input_review)\n",
        "\n",
        "    for word in input_words:\n",
        "      ttl_adj_words += 1\n",
        "\n",
        "      if not is_word_exist(word, words):\n",
        "        ttl_words += 1\n",
        "        output_word = [word, score]\n",
        "        words.append(output_word)\n",
        "\n",
        "  print(\"Total Adjective Words: {}\".format(ttl_adj_words))\n",
        "  print(\"Total Final Words: {}\".format(ttl_words))\n",
        "\n",
        "  outdf = pd.DataFrame(words)\n",
        "  outdf.rename(columns={0: \"word\", 1:\"Score\"}, errors=\"raise\", inplace=True)\n",
        "  outdf.to_csv(export_file_name, index=False)\n",
        "\n",
        "  print(export_file_name + \" is created!\")\n",
        "\n",
        "  return outdf\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ty7G_f_S38nT",
        "colab_type": "text"
      },
      "source": [
        "### 4.0 GENERATE POSITIVE LEXICON SENTIMENT"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MpNdz3QStZe5",
        "colab_type": "code",
        "outputId": "5abe2985-5aa6-4e53-94c1-99c1d10b478f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "positive_df = pd.read_csv('politic_issues_positive_reviews.csv', index_col=None)  \n",
        "positive_df.rename(columns={\"Google Translate\": \"original\"}, errors=\"raise\", inplace=True)\n",
        "out_pos_df = create_lexicon_sentiment(positive_df, 1, \"lexicon_political_positive.csv\")"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total Adjective Words: 158\n",
            "Total Final Words: 104\n",
            "lexicon_political_positive.csv is created!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iqlJhx4f4Bnq",
        "colab_type": "text"
      },
      "source": [
        "### 5.0 GENERATE NEGATIVE LEXICON SENTIMENT\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8bV9xdcZoLGl",
        "colab_type": "code",
        "outputId": "c5267f95-451d-4b8f-ca0f-6b035e511bea",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "negative_df = pd.read_csv('politic_issues_negative_reviews.csv', index_col=None)  \n",
        "negative_df.rename(columns={\"Google Translate\": \"original\"}, errors=\"raise\", inplace=True)\n",
        "out_neg_df = create_lexicon_sentiment(negative_df, -1, \"lexicon_political_negative.csv\")"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total Adjective Words: 152\n",
            "Total Final Words: 100\n",
            "lexicon_political_negative.csv is created!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AfGSZAN4Ux8_",
        "colab_type": "text"
      },
      "source": [
        "### 6.0 GENERATE MASTER LEXICON SENTIMENT\n",
        "The code will combine both Positive and Negative into one master files. <b>due to some word are exist in both positive and negative, hence the code below will flag it as 0. User need to manually update the 0 to either 1 or -1 if needed</b>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OS1fwvn2COLk",
        "colab_type": "code",
        "outputId": "97a39743-52d1-44d7-d1f7-181d1f23c945",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 442
        }
      },
      "source": [
        "pos_list = out_pos_df.values.tolist()\n",
        "neg_list = out_neg_df.values.tolist()\n",
        "all_list = []\n",
        "\n",
        "print(\"pos_list: \" + str(len(pos_list)))\n",
        "print(\"neg_list: \" + str(len(neg_list)))\n",
        "print(\"all_list: \" + str(len(all_list)))\n",
        "\n",
        "print(\"- COMPARE POSITIVE IN NEGATIVE\")\n",
        "for first_item in pos_list:\n",
        "  selected_first_item = first_item[0].strip()\n",
        "  for second_item in neg_list:\n",
        "    selected_second_item = second_item[0].strip()\n",
        "    if selected_first_item == selected_second_item:\n",
        "      first_item = [selected_first_item, 0]\n",
        "      break\n",
        "  all_list.append(first_item)\n",
        "\n",
        "print(\"pos_list: \" + str(len(pos_list)))\n",
        "print(\"neg_list: \" + str(len(neg_list)))\n",
        "print(\"all_list: \" + str(len(all_list)))\n",
        "\n",
        "\n",
        "print(\"- COMPARE NEGATIVE IN MASTER\")\n",
        "for first_item in neg_list:\n",
        "  selected_first_item = first_item[0].strip()\n",
        "  \n",
        "  # IF THE WORD ALREADY ADDED\n",
        "  ifexit = False\n",
        "  for second_item in all_list:\n",
        "    selected_second_item = second_item[0].strip()\n",
        "    if selected_first_item == selected_second_item:\n",
        "      # BREAK THE LOOP IF ALREADY EXIST\n",
        "      ifexit = True\n",
        "      break\n",
        "  \n",
        "  if not ifexit:\n",
        "    all_list.append(first_item)\n",
        "\n",
        "print(\"pos_list: \" + str(len(pos_list)))\n",
        "print(\"neg_list: \" + str(len(neg_list)))\n",
        "print(\"all_list: \" + str(len(all_list)))\n",
        "\n",
        "alldf = pd.DataFrame(all_list) \n",
        "alldf.rename(columns={0: \"word\", 1:\"Score\"}, errors=\"raise\", inplace=True)\n",
        "print(alldf)\n",
        "alldf.to_csv(\"lexicon_political_master.csv\", index=False)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "pos_list: 104\n",
            "neg_list: 100\n",
            "all_list: 0\n",
            "- COMPARE POSITIVE IN NEGATIVE\n",
            "pos_list: 104\n",
            "neg_list: 100\n",
            "all_list: 104\n",
            "- COMPARE NEGATIVE IN MASTER\n",
            "pos_list: 104\n",
            "neg_list: 100\n",
            "all_list: 174\n",
            "           word  Score\n",
            "0          also      1\n",
            "1       cynical      1\n",
            "2    indigenous      0\n",
            "3         aware      1\n",
            "4    successful      0\n",
            "..          ...    ...\n",
            "169     umpteen     -1\n",
            "170    actually     -1\n",
            "171     exactly     -1\n",
            "172       moral     -1\n",
            "173   regularly     -1\n",
            "\n",
            "[174 rows x 2 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}
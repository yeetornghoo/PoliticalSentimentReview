{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TTTP6234 - Assignment1_P101447_HOOYT.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O-ZHDwP5nCmZ",
        "colab_type": "text"
      },
      "source": [
        "# Assignment #1\n",
        "## Topic: Political Reviews - Generate Lexicon Sentiment\n",
        "\n",
        "Name: **Hoo Yee Torng** </br>\n",
        "Matrix: **P101447**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E0NYnTZz2mXS",
        "colab_type": "text"
      },
      "source": [
        "## 1.0 IMPORT LIBRARY\n",
        "Import all the library needed and download the database for Spacy, NLTK Lemmatizer, Stopwords etc"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o0nn6Uw9m67n",
        "colab_type": "code",
        "outputId": "da16e474-8661-4827-b226-3e112bb5d44b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 199
        }
      },
      "source": [
        "import pandas as pd\n",
        "import nltk\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.stem import LancasterStemmer\n",
        "from nltk.corpus import stopwords\n",
        "import re\n",
        "import spacy\n",
        "\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2cciHekQrpqO",
        "colab_type": "text"
      },
      "source": [
        "## 2.0 TEXT PROCESSING FUNCTION\n",
        "Define text processig function \n",
        "* extract_words - Only return alphebert words in lower case\n",
        "* get_adjective - Use both Spacy and NTLK to return adjective words\n",
        "* lemmatized_remove_stopword - Lemmatized the words and remove stop words\n",
        "* is_word_exist - To check if the word exist in the word list\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AhdNDhfz23I8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# RETURN WORD ONLY\n",
        "def extract_words(setence):\n",
        "  letters = re.sub(\"[^a-zA-Z]\", \" \", setence)\n",
        "  return(letters.lower())\n",
        "\n",
        "\n",
        "# GET ADJECTIVE ONLY\n",
        "# USE BOTH SPACY AND NTLK\n",
        "def get_adjective_word(setence):\n",
        "  words = []\n",
        "\n",
        "  ## CHECK BY SPACY FIRST\n",
        "  spacy_words = nlp(u''+setence+'')\n",
        "  for token in spacy_words:\n",
        "    if token.pos_ == 'ADJ':\n",
        "      #print(\"SPACY: {}:{}\".format(token.text, token.pos_))\n",
        "      words.append(token.text)\n",
        "\n",
        "  ## CHECK BY NLTK\n",
        "  nltk_words = nltk.word_tokenize(setence)\n",
        "  for stc in nltk_words:\n",
        "    nltk_token = nltk.pos_tag(nltk.word_tokenize(stc))\n",
        "    if nltk_token[0][1] == 'JJ' and nltk_token[0][0] not in words:\n",
        "      #print(\"NTLK : {}:{}\".format(nltk_token[0][0], nltk_token[0][1]))\n",
        "      words.append(nltk_token[0][0])\n",
        "    \n",
        "  return words\n",
        "\n",
        "\n",
        "# LEMMATIZED AND REMOVE STOP WORD\n",
        "def lemmatized_remove_stopword(sentence):\n",
        "    new_line = []\n",
        "    word_tokens = nltk.word_tokenize(sentence)\n",
        "    for w in word_tokens:\n",
        "      output_word = lemmatizer.lemmatize(w)\n",
        "      if output_word not in stop_words:\n",
        "        new_line.append(output_word)\n",
        "    \n",
        "    return (\" \".join(new_line) + \" \").strip()\n",
        "\n",
        "def is_word_exist(word, words):\n",
        "  for i in range(len(words)):\n",
        "    savedword = words[i][0].strip()\n",
        "    if savedword == word.strip():\n",
        "      return True\n",
        "  \n",
        "  return False"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ScOE10rc3sIz",
        "colab_type": "text"
      },
      "source": [
        "### 3.0 GENERATE LEXICON SENTIMENT FILE\n",
        "This is the function to generate the lexicon sentiment csv file by using following steps\n",
        "\n",
        "1. Extract only words from the sentence\n",
        "2. Remove all stopwords\n",
        "3. Return only adjective words\n",
        "4. Assign +1 to the words from Positive Review file and -1 for Negative Review. Only keep unique words in the same files.\n",
        "5. Saved the dataframe to csv file\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YLRj1pC4tZca",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_lexicon_sentiment(source_df, score, export_file_name):\n",
        "\n",
        "  words = []\n",
        "\n",
        "  ttl_adj_words = 0\n",
        "  ttl_words = 0\n",
        "\n",
        "  for i in range(len(source_df)):\n",
        "    input_review = source_df.loc[i, \"original\"]\n",
        "    input_review = extract_words(input_review)\n",
        "    input_review = lemmatized_remove_stopword(input_review)\n",
        "    input_words = get_adjective_word(input_review)\n",
        "\n",
        "    for word in input_words:\n",
        "      ttl_adj_words += 1\n",
        "\n",
        "      if not is_word_exist(word, words):\n",
        "        ttl_words += 1\n",
        "        output_word = [word, score]\n",
        "        words.append(output_word)\n",
        "\n",
        "  print(\"Total Adjective Words: {}\".format(ttl_adj_words))\n",
        "  print(\"Total Final Words: {}\".format(ttl_words))\n",
        "\n",
        "  outdf = pd.DataFrame(words)\n",
        "  outdf.rename(columns={0: \"word\", 1:\"Score\"}, errors=\"raise\", inplace=True)\n",
        "  outdf.to_csv(export_file_name, index=False)\n",
        "\n",
        "  print(export_file_name + \" is created!\")\n",
        "\n",
        "  return outdf\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ty7G_f_S38nT",
        "colab_type": "text"
      },
      "source": [
        "### 4.0 GENERATE POSITIVE LEXICON SENTIMENT\n",
        "Download [politic_issues_positive_reviews.csv](https://drive.google.com/drive/folders/1XgpcZJrtyJs9GxccH2TfJHSruvHFjJHg?usp=sharing) and upload it to this workspace"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MpNdz3QStZe5",
        "colab_type": "code",
        "outputId": "d324abb1-eda0-4197-f834-c1026a9c258b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        }
      },
      "source": [
        "positive_df = pd.read_csv('politic_issues_positive_reviews.csv', index_col=None)  \n",
        "positive_df.rename(columns={\"Google Translate\": \"original\"}, errors=\"raise\", inplace=True)\n",
        "out_pos_df = create_lexicon_sentiment(positive_df, 1, \"lexicon_political_positive.csv\")"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total Adjective Words: 164\n",
            "Total Final Words: 110\n",
            "lexicon_political_positive.csv is created!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iqlJhx4f4Bnq",
        "colab_type": "text"
      },
      "source": [
        "### 5.0 GENERATE NEGATIVE LEXICON SENTIMENT\n",
        "Download [politic_issues_negative_reviews.csv](https://drive.google.com/drive/folders/1XgpcZJrtyJs9GxccH2TfJHSruvHFjJHg?usp=sharing) and upload it to this workspace\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8bV9xdcZoLGl",
        "colab_type": "code",
        "outputId": "18cbb6a7-367d-439c-f38a-3b0ee462aa58",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        }
      },
      "source": [
        "negative_df = pd.read_csv('politic_issues_negative_reviews.csv', index_col=None)  \n",
        "negative_df.rename(columns={\"Google Translate\": \"original\"}, errors=\"raise\", inplace=True)\n",
        "out_neg_df = create_lexicon_sentiment(negative_df, -1, \"lexicon_political_negative.csv\")"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total Adjective Words: 196\n",
            "Total Final Words: 127\n",
            "lexicon_political_negative.csv is created!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AfGSZAN4Ux8_",
        "colab_type": "text"
      },
      "source": [
        "### 6.0 GENERATE MASTER LEXICON SENTIMENT\n",
        "The code will combine both Positive and Negative into one master files. <b>due to some word are exist in both positive and negative, hence the code below will flag it as 0. User need to manually update the 0 to either 1 or -1 if needed</b>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OS1fwvn2COLk",
        "colab_type": "code",
        "outputId": "e9090289-7d2f-40ee-8c62-d096700eb0cd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 472
        }
      },
      "source": [
        "pos_list = out_pos_df.values.tolist()\n",
        "neg_list = out_neg_df.values.tolist()\n",
        "all_list = []\n",
        "\n",
        "print(\"pos_list: \" + str(len(pos_list)))\n",
        "print(\"neg_list: \" + str(len(neg_list)))\n",
        "print(\"all_list: \" + str(len(all_list)))\n",
        "\n",
        "print(\"- COMPARE POSITIVE IN NEGATIVE\")\n",
        "for first_item in pos_list:\n",
        "  selected_first_item = first_item[0].strip()\n",
        "  for second_item in neg_list:\n",
        "    selected_second_item = second_item[0].strip()\n",
        "    if selected_first_item == selected_second_item:\n",
        "      first_item = [selected_first_item, 0]\n",
        "      break\n",
        "  all_list.append(first_item)\n",
        "\n",
        "print(\"pos_list: \" + str(len(pos_list)))\n",
        "print(\"neg_list: \" + str(len(neg_list)))\n",
        "print(\"all_list: \" + str(len(all_list)))\n",
        "\n",
        "\n",
        "print(\"- COMPARE NEGATIVE IN MASTER\")\n",
        "for first_item in neg_list:\n",
        "  selected_first_item = first_item[0].strip()\n",
        "  \n",
        "  # IF THE WORD ALREADY ADDED\n",
        "  ifexit = False\n",
        "  for second_item in all_list:\n",
        "    selected_second_item = second_item[0].strip()\n",
        "    if selected_first_item == selected_second_item:\n",
        "      # BREAK THE LOOP IF ALREADY EXIST\n",
        "      ifexit = True\n",
        "      break\n",
        "  \n",
        "  if not ifexit:\n",
        "    all_list.append(first_item)\n",
        "\n",
        "print(\"pos_list: \" + str(len(pos_list)))\n",
        "print(\"neg_list: \" + str(len(neg_list)))\n",
        "print(\"all_list: \" + str(len(all_list)))\n",
        "\n",
        "alldf = pd.DataFrame(all_list) \n",
        "alldf.rename(columns={0: \"word\", 1:\"Score\"}, errors=\"raise\", inplace=True)\n",
        "print(alldf)\n",
        "alldf.to_csv(\"lexicon_political_master.csv\", index=False)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "pos_list: 110\n",
            "neg_list: 127\n",
            "all_list: 0\n",
            "- COMPARE POSITIVE IN NEGATIVE\n",
            "pos_list: 110\n",
            "neg_list: 127\n",
            "all_list: 110\n",
            "- COMPARE NEGATIVE IN MASTER\n",
            "pos_list: 110\n",
            "neg_list: 127\n",
            "all_list: 206\n",
            "           word  Score\n",
            "0       sarcasm      1\n",
            "1      conveyed      1\n",
            "2       cynical      1\n",
            "3    indigenous      0\n",
            "4         aware      1\n",
            "..          ...    ...\n",
            "201    national     -1\n",
            "202     umpteen     -1\n",
            "203       moral     -1\n",
            "204     worrior     -1\n",
            "205        long     -1\n",
            "\n",
            "[206 rows x 2 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}